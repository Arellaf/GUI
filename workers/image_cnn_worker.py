import time
import json
import os
import math
import numpy as np
from PyQt5.QtCore import QThread, pyqtSignal
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
class ImageNet_CNN_Worker(QThread):
    """
    QThread-Ñ€Ð¾Ð±Ñ–Ñ‚Ð½Ð¸Ðº Ð´Ð»Ñ Ñ‚Ñ€ÐµÐ½ÑƒÐ²Ð°Ð½Ð½Ñ Ð¼Ð¾Ð´ÐµÐ»Ñ–.
    ÐžÑ‡Ñ–ÐºÑƒÑ”Ñ‚ÑŒÑÑ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð°:
    dataset_path/
       train/
         dogs/
         cats/
       val/
         dogs/
         cats/
    """
    progress = pyqtSignal(str)
    finished = pyqtSignal(str, object, dict)
    def __init__(self, dataset_path, epochs=10, img_size=128, batch_size=128, layers=None):
        super().__init__()
        self.dataset_path = dataset_path
        self.epochs = epochs
        self.img_size = img_size
        self.batch_size = batch_size
        self.layers = layers or []
        self.model = None
        self.class_indices = None
    # --------------------------------------------------------------
    # BUILD MODEL
    # --------------------------------------------------------------
    def build_model(self, input_shape, num_classes):
        """
        ÐŸÑ€Ð¾ÑÑ‚Ð¸Ð¹, Ð°Ð»Ðµ ÐµÑ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¸Ð¹ CNN Ð· BatchNorm Ñ– Dropout.
        ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¸ dense-ÑÐ»Ð¾Ñ—Ð² Ð¼Ð¾Ð¶Ð½Ð° Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‚Ð¸ Ñ‡ÐµÑ€ÐµÐ· self.layers (ÑÐ¿Ð¸ÑÐ¾Ðº dict Ð· keys: units, activation).
        """
        model = Sequential()
        model.add(Conv2D(32, (3, 3), activation="relu", padding="same", input_shape=input_shape))
        model.add(BatchNormalization())
        model.add(MaxPooling2D(2, 2))
        model.add(Conv2D(64, (3, 3), activation="relu", padding="same"))
        model.add(BatchNormalization())
        model.add(MaxPooling2D(2, 2))
        model.add(Conv2D(128, (3, 3), activation="relu", padding="same"))
        model.add(BatchNormalization())
        model.add(MaxPooling2D(2, 2))
        model.add(Flatten())
        model.add(Dropout(0.4))
        # custom dense layers
        for layer in self.layers:
            units = int(layer.get("units", 128))
            activation = layer.get("activation", "relu")
            model.add(Dense(units, activation=activation))
        # final classifier
        model.add(Dense(num_classes, activation="softmax"))
        model.compile(
            optimizer=Adam(learning_rate=0.0005),
            loss="categorical_crossentropy",
            metrics=["accuracy"]
        )
        return model
    # --------------------------------------------------------------
    # TRAIN
    # --------------------------------------------------------------
    def run(self):
        try:
            self.progress.emit("ðŸ“¥ ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸ Ð¿Ð°Ð¿Ð¾Ðº Ñ‚Ð° Ð¿Ñ–Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð°Ð½Ð¸Ñ…...")
            # ÐžÑ‡Ñ–ÐºÑƒÑ”Ð¼Ð¾ dataset_path Ð¼Ð°Ñ” Ð¿Ñ–Ð´Ð¿Ð°Ð¿ÐºÐ¸ train/ Ñ– val/
            train_dir = os.path.join(self.dataset_path, "train")
            val_dir = os.path.join(self.dataset_path, "val")
            if not os.path.isdir(train_dir) or not os.path.isdir(val_dir):
                raise FileNotFoundError("ÐŸÐ°Ð¿ÐºÐ¸ 'train' Ñ– 'val' Ð¼Ð°ÑŽÑ‚ÑŒ Ñ–ÑÐ½ÑƒÐ²Ð°Ñ‚Ð¸ Ð²ÑÐµÑ€ÐµÐ´Ð¸Ð½Ñ– Ð²Ð¸Ð±Ñ€Ð°Ð½Ð¾Ð³Ð¾ ÐºÐ°Ñ‚Ð°Ð»Ð¾Ð³Ñƒ.")
            # ÐÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ñ–Ñ Ð´Ð»Ñ Ñ‚Ñ€ÐµÐ½ÑƒÐ²Ð°Ð½Ð½Ñ
            train_datagen = ImageDataGenerator(
                rescale=1.0 / 255.0,
                rotation_range=20,
                width_shift_range=0.1,
                height_shift_range=0.1,
                shear_range=0.1,
                zoom_range=0.1,
                horizontal_flip=True,
                fill_mode="nearest"
            )
            val_datagen = ImageDataGenerator(rescale=1.0 / 255.0)
            train = train_datagen.flow_from_directory(
                train_dir,
                target_size=(self.img_size, self.img_size),
                class_mode="categorical",
                batch_size=self.batch_size,
                shuffle=True
            )
            val = val_datagen.flow_from_directory(
                val_dir,
                target_size=(self.img_size, self.img_size),
                class_mode="categorical",
                batch_size=self.batch_size,
                shuffle=False
            )
            if train.samples == 0 or val.samples == 0:
                raise ValueError("ÐÐµÐ¼Ð°Ñ” Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½ÑŒ Ñƒ train Ð°Ð±Ð¾ val. ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€Ñ‚Ðµ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð¿Ð°Ð¿Ð¾Ðº Ñ– Ñ„Ð°Ð¹Ð»Ð¸.")
            self.class_indices = train.class_indices
            num_classes = len(self.class_indices)
            input_shape = (self.img_size, self.img_size, 3)
            self.progress.emit(f"ðŸ“Š ÐšÐ»Ð°ÑÑ–Ð²: {num_classes} â€” {self.class_indices}")
            self.progress.emit(f"Train samples: {train.samples}, Val samples: {val.samples}")
            self.model = self.build_model(input_shape, num_classes)
            # Ð¿Ñ–Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° callback'Ñ–Ð²: Ð·Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ Ð½Ð°Ð¹ÐºÑ€Ð°Ñ‰Ð¾Ñ— Ð¼Ð¾Ð´ÐµÐ»Ñ– Ð¿Ð¾ val_loss, Ð·Ð½Ð¸Ð¶ÐµÐ½Ð½Ñ lr Ñ‚Ð° Ñ€Ð°Ð½Ð½Ñ Ð·ÑƒÐ¿Ð¸Ð½ÐºÐ°
            tmp_model_path = os.path.join(self.dataset_path, "best_model_temp.h5")
            checkpoint = ModelCheckpoint(tmp_model_path, monitor="val_loss", save_best_only=True, verbose=0)
            reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3, verbose=0)
            early_stop = EarlyStopping(monitor="val_loss", patience=8, restore_best_weights=True, verbose=0)
            steps_per_epoch = math.ceil(train.samples / float(self.batch_size))
            validation_steps = math.ceil(val.samples / float(self.batch_size))
            start = time.time()
            # Ð¢Ñ€ÐµÐ½ÑƒÑ”Ð¼Ð¾ ÐµÐ¿Ð¾Ñ…Ð°Ð¼Ð¸, Ð°Ð»Ðµ ÐµÐ¼ÑƒÐ»ÑŽÑ”Ð¼Ð¾ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑ Ð¿Ð¾-ÐµÐ¿Ð¾Ñ…Ð°Ð¼ (Ñ‰Ð¾Ð± Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÑÑ‚Ð¸ UI)
            history = self.model.fit(
                train,
                epochs=self.epochs,
                validation_data=val,
                steps_per_epoch=steps_per_epoch,
                validation_steps=validation_steps,
                callbacks=[checkpoint, reduce_lr, early_stop],
                verbose=0
            )
            train_time = time.time() - start
            # Ð¯ÐºÑ‰Ð¾ callback Ð·Ð±ÐµÑ€Ñ–Ð³ ÐºÑ€Ð°Ñ‰Ñƒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñƒ Ñ‚Ð¸Ð¼Ñ‡Ð°ÑÐ¾Ð²Ð¸Ð¹ Ñ„Ð°Ð¹Ð», Ð¿Ñ–Ð´Ð²Ð°Ð½Ñ‚Ð°Ð¶Ð¸Ð¼Ð¾ Ñ—Ñ—
            if os.path.exists(tmp_model_path):
                try:
                    best = load_model(tmp_model_path)
                    self.model = best
                    # Ð²Ð¸Ð´Ð°Ð»ÑÑ‚Ð¸ Ñ‚Ð¸Ð¼Ñ‡Ð°ÑÐ¾Ð²Ð¸Ð¹ Ñ„Ð°Ð¹Ð» Ð½Ðµ Ð¾Ð±Ð¾Ð²'ÑÐ·ÐºÐ¾Ð²Ð¾
                except Exception:
                    pass
            # Ð¤Ð¾Ñ€Ð¼ÑƒÐ²Ð°Ð½Ð½Ñ Ð¿Ñ–Ð´ÑÑƒÐ¼ÐºÐ¾Ð²Ð¾Ð³Ð¾ Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð½Ñ Ð· Ð¾ÑÑ‚Ð°Ð½Ð½Ñ–Ñ… Ð¼ÐµÑ‚Ñ€Ð¸Ðº
            last_acc = history.history.get("accuracy", [None])[-1]
            last_val_acc = history.history.get("val_accuracy", [None])[-1]
            last_loss = history.history.get("loss", [None])[-1]
            last_val_loss = history.history.get("val_loss", [None])[-1]
            done = (
                f"ðŸŽ‰ ÐÐ°Ð²Ñ‡Ð°Ð½Ð½Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾! Ð§Ð°Ñ: {train_time:.2f} ÑÐµÐº. "
                f"acc={last_acc:.4f} val_acc={last_val_acc:.4f} "
                f"loss={last_loss:.4f} val_loss={last_val_loss:.4f}"
            )
            # ÐŸÐ¾Ð²ÐµÑ€Ñ‚Ð°Ñ”Ð¼Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ– class_indices
            self.finished.emit(done, self.model, self.class_indices)
        except Exception as e:
            # ÐŸÑ€Ð¸ Ð¿Ð¾Ð¼Ð¸Ð»Ñ†Ñ– Ð¿Ð¾Ð²ÐµÑ€Ñ‚Ð°Ñ”Ð¼Ð¾ Ñ‚ÐµÐºÑÑ‚ Ð¿Ð¾Ð¼Ð¸Ð»ÐºÐ¸
            self.finished.emit(f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ°: {e}", None, None)
    # --------------------------------------------------------------
    # SAVE MODEL + CLASSES
    # --------------------------------------------------------------
    @staticmethod
    def save_model_full(model, class_indices, path):
        """
        Ð—Ð±ÐµÑ€ÐµÐ³Ñ‚Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ (h5 Ð°Ð±Ð¾ .keras) Ñ‚Ð° Ñ„Ð°Ð¹Ð» classes.json Ð¿Ð¾Ñ€ÑƒÑ‡ (path + "_classes.json").
        """
        # ensure dir exists
        dest_dir = os.path.dirname(path) or "."
        os.makedirs(dest_dir, exist_ok=True)
        model.save(path)
        with open(path + "_classes.json", "w", encoding="utf-8") as f:
            # Ð·Ð±ÐµÑ€Ñ–Ð³Ð°Ñ”Ð¼Ð¾ mapping label->index
            json.dump(class_indices, f, ensure_ascii=False, indent=2)
    # --------------------------------------------------------------
    # LOAD MODEL + CLASSES
    # --------------------------------------------------------------
    @staticmethod
    def load_model_full(path):
        model = load_model(path)
        with open(path + "_classes.json", "r", encoding="utf-8") as f:
            loaded = json.load(f)
        # ÐŸÑ€Ð¸Ð²Ð¾Ð´Ð¸Ð¼Ð¾ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ Ð´Ð¾ int
        class_indices = {k: int(v) for k, v in loaded.items()}
        return model, class_indices
    # --------------------------------------------------------------
    # PREDICT IMAGE
    # --------------------------------------------------------------
    @staticmethod
    def predict_image(model, class_indices, file_path, img_size=128):
        """
        ÐŸÐ¾Ð²ÐµÑ€Ñ‚Ð°Ñ” (label, probability)
        class_indices: dict label -> index, ÑÐº Ð· flow_from_directory
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: {file_path}")
        img = load_img(file_path, target_size=(img_size, img_size))
        arr = img_to_array(img) / 255.0
        arr = np.expand_dims(arr, axis=0)
        pred = model.predict(arr)[0]
        idx = int(np.argmax(pred))
        # Ñ–Ð½Ð²ÐµÑ€Ñ‚ÑƒÑ”Ð¼Ð¾ class_indices (index -> label)
        classes = {v: k for k, v in class_indices.items()}
        label = classes.get(idx, "unknown")
        prob = float(pred[idx])
        return label, prob